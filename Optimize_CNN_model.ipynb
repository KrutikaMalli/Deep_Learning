{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Creating a CNN Model and optimize it using Keras Tuner**"
      ],
      "metadata": {
        "id": "TXL6UFCc1PEg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Hyperparameters in CNN\n",
        "Hyperparameters in a Convolutional Neural Network (CNN) are parameters that are set before training the model and determine how the model is trained. These parameters are not learned from the data during training but are chosen by the data scientist or machine learning practitioner before training begins. Proper tuning of hyperparameters can significantly impact the performance of your CNN. Here are some common hyperparameters in CNNs:\n",
        "\n",
        "Learning Rate: This is one of the most crucial hyperparameters. It determines the step size at which the optimizer adjusts the model's weights during training. Too high a learning rate can cause the optimization process to diverge, while too low a learning rate can lead to slow convergence. It needs to be tuned carefully.\n",
        "\n",
        "Number of Convolutional Layers: The architecture of your CNN includes decisions about how many convolutional layers to use. Deeper networks may capture more complex features but could lead to overfitting, while shallower networks might not capture enough features.\n",
        "\n",
        "Number of Filters/Kernels: Each convolutional layer consists of multiple filters (also known as kernels) that scan the input data to detect different features. The number of filters in each layer affects the network's capacity to learn complex patterns.\n",
        "\n",
        "Filter Size: The size of the filters determines the spatial extent of the features they can detect. Common filter sizes are 3x3, 5x5, and 7x7. Smaller filter sizes generally capture finer details, while larger filter sizes capture more global features.\n",
        "\n",
        "Pooling Type and Size: Pooling layers downsample the spatial dimensions of the feature maps, reducing computation and helping to generalize. Common pooling operations are max pooling and average pooling. The pooling size determines how much downsampling is applied.\n",
        "\n",
        "Activation Functions: Activation functions introduce non-linearity into the network. Common activation functions include ReLU (Rectified Linear Unit), sigmoid, and tanh. ReLU is often preferred due to its simplicity and effectiveness.\n",
        "\n",
        "Dropout Rate: Dropout is a regularization technique that randomly sets a fraction of input units to zero during each update. It helps prevent overfitting by reducing interdependence between neurons.\n",
        "\n",
        "Batch Size: The number of training examples used in each iteration of gradient descent. Larger batch sizes may lead to faster convergence but could require more memory.\n",
        "\n",
        "Number of Epochs: An epoch is one complete iteration through the entire training dataset. Too few epochs might not allow the model to learn properly, while too many could lead to overfitting.\n",
        "\n",
        "Optimizer Choice: Different optimization algorithms like SGD (Stochastic Gradient Descent), Adam, RMSProp, etc., have different behaviors and learning rate adaptivity.\n",
        "\n",
        "Weight Initialization: The initial values of the weights can affect the convergence of the optimization process. Proper weight initialization strategies can help the model train faster and more effectively.\n",
        "\n",
        "Learning Rate Schedule: Adjusting the learning rate during training can help balance rapid progress in the beginning with finer adjustments as the model converges.\n",
        "\n",
        "Tuning these hyperparameters often involves a combination of trial and error, intuition, and sometimes automated techniques like grid search, random search, or Bayesian optimization. It's important to note that finding the best hyperparameters is a part of the model development process and requires experimentation and understanding of the specific problem and dataset you're working with."
      ],
      "metadata": {
        "id": "LZTgX5d5Zv1C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset:** Fashion_mnist dataset taken from Keras.\n",
        "\n",
        "**Check the Runtime is in GPU** and not CPU."
      ],
      "metadata": {
        "id": "oofir9Dr3qI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1U2rsSC1LvE",
        "outputId": "d69a6677-f8f6-4cf5-e197-7acaf55de2bd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.8.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner\n",
        "\n",
        "#helps us choose how many convolutional layers we need in the model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFFGj49I1Lsw",
        "outputId": "3b61fe64-7fec-4bab-8881-1b1647cb43b9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (3.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.32.3)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (13.8.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (3.11.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.12.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras->keras-tuner) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras-tuner) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras-tuner) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n",
            "Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*inside sequential layer, we adding conolutional 2D layer along with flatten and dense layer.\n",
        "\n",
        "* First parameter is filters:\n",
        "- with the help of keras_tuner, we can select diff values for filters(hp.Int will creating a range of values between 32 and 128.\n",
        "- kernel_size is nothing but filter size( we can hp choice preferred)\n",
        "\n",
        "* similar operations on performed on next layer keeping min_size = 32 and max_size = 64.\n",
        "\n",
        "\n",
        "* Next we flatten the layers, and for the dense layer ..we enter min and max value.\n",
        "\n",
        "* At last we adding the end dense layer with 10 output nodes and ativation used here for ths is Softmax. We will get the output in the form of probabilities.\n",
        "\n",
        "\n",
        "* In compilation, we used the optimizer - Adam, using hp.choice to choose the learning rate between the specified values only."
      ],
      "metadata": {
        "id": "50zjNLts6cuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator   #ImageDataGenerator is for applying data augmentation.\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from kerastuner import RandomSearch                                   #RandomSearch from Keras Tuner is used for hyperparameter tuning.\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "GOF-aQNz2Y8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Fashion MNIST dataset\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n"
      ],
      "metadata": {
        "id": "B6YxDaOg2bPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape the images to add a channel dimension\n",
        "train_images = train_images.reshape(len(train_images), 28, 28, 1).astype('float32') / 255.0\n",
        "test_images = test_images.reshape(len(test_images), 28, 28, 1).astype('float32') / 255.0"
      ],
      "metadata": {
        "id": "B5tfCAZG2dgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split training data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_images, train_labels, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "Rlm_zC8w2fVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")"
      ],
      "metadata": {
        "id": "jIn3AxUV2iF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "def build_model(hp):\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Conv2D(\n",
        "            filters=hp.Int('conv_1_filter', min_value=32, max_value=128, step=16),\n",
        "            kernel_size=hp.Choice('conv_1_kernel', values=[3, 5]),\n",
        "            activation='relu',\n",
        "            input_shape=(28, 28, 1)\n",
        "        ),\n",
        "        keras.layers.Conv2D(\n",
        "            filters=hp.Int('conv_2_filter', min_value=32, max_value=128, step=16),\n",
        "            kernel_size=hp.Choice('conv_2_kernel', values=[3, 5]),\n",
        "            activation='relu'\n",
        "        ),\n",
        "        keras.layers.Flatten(),\n",
        "        keras.layers.Dense(\n",
        "            units=hp.Int('dense_1_units', min_value=32, max_value=128, step=16),\n",
        "            activation='relu'\n",
        "        ),\n",
        "        keras.layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3])),\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "1SDku7an2k4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Hyperparameter tuning using Random Search\n",
        "tuner_search = RandomSearch(build_model, objective='val_accuracy',\n",
        "                            max_trials=5, directory='output',\n",
        "                            project_name=\"Mnist_Fashion\")"
      ],
      "metadata": {
        "id": "0qGYwqnB2uHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the tuner\n",
        "tuner_search.search(X_train, y_train, epochs=5, validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "id": "2fC_9WQG2vuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the best model\n",
        "model = tuner_search.get_best_models(num_models=1)[0]\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "3bHXRg5Y2xmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3)"
      ],
      "metadata": {
        "id": "pd7NCsbg2zJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrain the model using the best parameters with increased epochs\n",
        "r = model.fit(datagen.flow(X_train, y_train, batch_size=32),\n",
        "               epochs=10,  # Increased number of epochs\n",
        "               validation_data=(X_val, y_val),\n",
        "               callbacks=[early_stopping])"
      ],
      "metadata": {
        "id": "BodstvPz219z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the training and validation accuracy\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(r.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(r.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "# Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(r.history['loss'], label='Train Loss')\n",
        "plt.plot(r.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "oAy58Krs235e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy Graph:\n",
        "\n",
        "You should see the training accuracy increase over time as the model learns from the data.\n",
        "The validation accuracy should ideally increase as well, indicating that the model generalizes well to unseen data.\n",
        "If the training accuracy continues to increase while the validation accuracy plateaus or decreases, it could indicate overfitting.\n",
        "\n",
        "Loss Graph:\n",
        "\n",
        "The training loss should decrease over epochs, indicating that the model is learning effectively.\n",
        "The validation loss should also decrease. If it starts to increase after a point while training loss continues to decrease, it suggests that the model is starting to overfit to the training data."
      ],
      "metadata": {
        "id": "8VK39AQwmtkV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**\n",
        "This code effectively implements a convolutional neural network to classify images from the Fashion MNIST dataset. The integration of data augmentation, hyperparameter tuning, and early stopping helps improve model performance and generalization. By examining the resulting graphs, you can evaluate how well the model is performing during training and validation, helping you make informed decisions about further tuning and adjustments."
      ],
      "metadata": {
        "id": "e599ATHamjtn"
      }
    }
  ]
}